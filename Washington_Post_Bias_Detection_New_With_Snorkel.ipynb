{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf9c5505",
   "metadata": {
    "toc": true
   },
   "source": [
    " Todo: Aktualisieren:\n",
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\">\n",
    "    <li><span><a href=\"#|-Preliminaries\" data-toc-modified-id=\"|-Preliminaries-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>| Preliminaries</a></span></li><li><span><a href=\"#|-Cleaning\" data-toc-modified-id=\"|-Cleaning-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>| Cleaning</a></span></li><li><span><a href=\"#|-Named-Entity-Recognition-w/-spaCy\" data-toc-modified-id=\"|-Named-Entity-Recognition-w/-spaCy-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>| Named Entity Recognition w/ spaCy</a></span><ul class=\"toc-item\"><li><span><a href=\"#Split-first,-middle-and-last-name-in-different-cols\" data-toc-modified-id=\"Split-first,-middle-and-last-name-in-different-cols-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Split first, middle and last name in different cols</a></span></li><li><span><a href=\"#|-Enrich-first-name-if-only-last-name-is-mentioned\" data-toc-modified-id=\"|-Enrich-first-name-if-only-last-name-is-mentioned-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>| Enrich first name if only last name is mentioned</a></span></li></ul></li><li><span><a href=\"#|-Gender-Guesser\" data-toc-modified-id=\"|-Gender-Guesser-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>| Gender Guesser</a></span></li><li><span><a href=\"#|-Group-per-article-and-get-share-of-gender-per-article\" data-toc-modified-id=\"|-Group-per-article-and-get-share-of-gender-per-article-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>| Group per article and get share of gender per article</a></span></li><li><span><a href=\"#|-Wikidata-Query\" data-toc-modified-id=\"|-Wikidata-Query-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>| Wikidata Query</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5998ed56",
   "metadata": {},
   "source": [
    "https://github.com/julesellaboranto/washington_post_bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a599aa92",
   "metadata": {},
   "source": [
    "---\n",
    "# | Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2255e98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import regex as re\n",
    "from pandarallel import pandarallel\n",
    "import pathlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#from bs4 import BeautifulSoup as bs\n",
    "\n",
    "# SpaCy \n",
    "import spacy\n",
    "import en_core_web_trf\n",
    "import en_core_web_md\n",
    "import en_core_web_sm\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "\n",
    "# TRF Orininal / Md schneller\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "# JULIA nlp = spacy.load(\"en_core_web_trf\")\n",
    "\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Span\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # (action='once')\n",
    "\n",
    "import tqdm\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd46d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "pandarallel.initialize(progress_bar=True)\n",
    "#pandarallel.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91dd141",
   "metadata": {},
   "source": [
    "---\n",
    "# | FILL OUTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46325819",
   "metadata": {},
   "outputs": [],
   "source": [
    "myWishedDatasets = 100000\n",
    "myWishedSmallFiles = 30\n",
    "myTRECfile = 'C:\\\\Users\\\\Felix\\\\github\\\\privat\\\\dis25_abgabe\\\\data\\\\wpdata\\\\data\\\\TREC_Washington_Post_collection.v4.jl'\n",
    "myZwischenSpeichern = False\n",
    "myDataPath = 'C:\\\\Users\\\\Felix\\\\github\\\\privat\\\\dis25_abgabe\\\\data\\\\wpdata\\\\data'\n",
    "\n",
    "zeitanfang = time.time()\n",
    "print(f'Programmstart bei {myWishedDatasets} Datens√§tzen ----------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be352a3a",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---\n",
    "# | Pre Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3a606e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Speichern des aktuellen Stands wenn gewollt\n",
    "def zwischenSpeichern(path: str, version: str):\n",
    "    zwischenstandPath = os.path.join(myDataPath, 'zwischenstand')\n",
    "    MyPickleFile = f'zwischenstand_{version}.pickle'\n",
    "    MyPickleFile = os.path.join(zwischenstandPath, MyPickleFile)\n",
    "    \n",
    "    if not os.path.exists(zwischenstandPath):\n",
    "        os.makedirs(zwischenstandPath)\n",
    "        print(f'{zwischenstandPath} wurde erstellt.')\n",
    "    \n",
    "    zwischenstandFiles = os.listdir(zwischenstandPath)\n",
    "    if not zwischenstandFiles:\n",
    "        df.to_pickle(MyPickleFile)\n",
    "    else:\n",
    "        for zwischenstandFile in zwischenstandFiles:\n",
    "            zwischenstandFile = os.path.join(zwischenstandPath, zwischenstandFile)\n",
    "            os.remove(zwischenstandFile)\n",
    "        df.to_pickle(MyPickleFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f52471",
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitFile(TRECfile, quantity):\n",
    "    \"\"\"\n",
    "    TREC_Washington_Post_collection.v4.jl has 728626 Lines and has 15GB.\n",
    "    It is to large to read into one Dataframe for the most PCs.\n",
    "    With this function you can split the big TREC-File into as many\n",
    "    small files you want.\n",
    "    \n",
    "    args: \n",
    "    TRECfile = Complete Path + Filename\n",
    "    quantitiy = How many Small files you want to create\n",
    "    \"\"\"\n",
    "    \n",
    "    # Splitting the complete Path to Filename and Path-only\n",
    "    TRECpath = TRECfile.split('\\\\')\n",
    "    TRECpath = TRECfile[:(len(TRECpath[-1])*-1)-1]\n",
    "    \n",
    "    # Creating (if necessary) a subfolder 'small' for the new Files\n",
    "    TRECpathSmall = os.path.join(TRECpath, 'small')\n",
    "    if not os.path.exists(TRECpathSmall):\n",
    "        os.makedirs(TRECpathSmall)\n",
    "        print(f'{TRECpathSmall} wurde erstellt.')\n",
    "    \n",
    "    # If the folder 'small' already has splitted files: data skip the Split-function\n",
    "    TRECfileSmall = os.listdir(TRECpathSmall)\n",
    "    if not TRECfileSmall:\n",
    "        smallfiles = []\n",
    "        lines_per_file = int(729000/quantity)\n",
    "        smallfile = None\n",
    "        fileCounter = 1\n",
    "        try:\n",
    "            with open(TRECfile) as bigfile:\n",
    "                for lineno, line in enumerate(bigfile):\n",
    "                    if lineno % lines_per_file == 0:\n",
    "                        if smallfile:\n",
    "                            smallfile.close()\n",
    "                        small_filename = f'TREC_Washington_Post_small_{fileCounter}.jl'\n",
    "                        small_filename = os.path.join(TRECpathSmall, small_filename)\n",
    "                        print(f'{small_filename} wird erstellt')\n",
    "                        smallfiles.append(small_filename)\n",
    "                        fileCounter += 1\n",
    "                        smallfile = open(small_filename, \"w\")\n",
    "                    smallfile.write(line)\n",
    "                if smallfile:\n",
    "                    smallfile.close()\n",
    "            print(f'Es befinden {len(smallfiles)} JSON-Files im \"small\" Unterordner.')\n",
    "            # return a list of the splitted data\n",
    "            return smallfiles\n",
    "        except:\n",
    "            print('Path der TREC_Washington_Post_collection.v4.jl ist falsch\\nDatei wurde nicht gefunden.')\n",
    "    else:\n",
    "        # If the folder 'small' is already there and has data create a list of the data and return it\n",
    "        TRECfileSmall = os.listdir(TRECpathSmall)\n",
    "        smallfiles = []\n",
    "        for smallfile in TRECfileSmall:\n",
    "            smallfile = os.path.join(TRECpathSmall, smallfile)\n",
    "            smallfiles.append(smallfile)\n",
    "        print(f'Es befinden bereits {len(smallfiles)} Files im \"small\" Unterordner.')\n",
    "        return smallfiles\n",
    "\n",
    "WpDataSmall = splitFile(myTRECfile, myWishedSmallFiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098eb3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SmallFilesToPickle(listOfSmallFiles):\n",
    "    \"\"\"\n",
    "    Opens the splitted JSON-Files and converts these dataframes\n",
    "    into smaller sub-dataframes.\n",
    "    \n",
    "    args: \n",
    "    listOfSmallFiles = list of the splitted TREC File\n",
    "    \"\"\"\n",
    "    fileending = listOfSmallFiles[0]\n",
    "    fileending = pathlib.Path(fileending).suffix\n",
    "    fileending = fileending[1:]\n",
    "    # if fileending is .jl it can be read as a json\n",
    "    if fileending == 'jl':\n",
    "        listOfSmallFilesPickle = []\n",
    "        for filename in listOfSmallFiles:\n",
    "            df = pd.read_json(filename, lines=True)\n",
    "\n",
    "            # Drop empty rows\n",
    "            df.dropna(inplace = True)\n",
    "\n",
    "            # delete duplicated and unnecessary columns\n",
    "            if 'contents' in df.columns:\n",
    "                df = df.drop(columns='contents')\n",
    "            if 'type' in df.columns:\n",
    "                df = df.drop(columns='type')\n",
    "            if 'source' in df.columns:\n",
    "                df = df.drop(columns='source')\n",
    "            if 'orig-id' in df.columns:\n",
    "                df = df.drop(columns='orig-id')\n",
    "            if 'publish_date' in df.columns:\n",
    "                df = df.drop(columns='publish_date')\n",
    "                \n",
    "            filenamemeameOld = filename\n",
    "            fileNameNew = filename[:-3]+'.pickle'\n",
    "            os.remove(filenamemeameOld)\n",
    "            df.to_pickle(fileNameNew)\n",
    "            listOfSmallFilesPickle.append(fileNameNew)\n",
    "            del df\n",
    "        print(f'Es wurden {len(listOfSmallFiles)} JSON-Files nach Pickle convertiert.')\n",
    "        return listOfSmallFilesPickle\n",
    "    # if fileending is .pickle it already has been split and the list just getting returned\n",
    "    elif fileending == 'pickle':\n",
    "        print(f'Es befinden {len(listOfSmallFiles)} Pickle-Files im \"small\" Unterordner.')\n",
    "        return listOfSmallFiles\n",
    "    else:\n",
    "        # if fileending is an unusual one nothing happens\n",
    "        print('Es befinden sich weder JSON- noch Pickle-Files im \"small\" Unterordner.')\n",
    "        return None\n",
    "\n",
    "WpDataSmall = SmallFilesToPickle(WpDataSmall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530774ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------- L√ñSCHEN WENN ANDERE FILES GENUTZT WERDEN SOLLEN  ------------------------\n",
    "\n",
    "hundredthousandWPpath = 'C:\\\\Users\\\\Felix\\\\github\\\\privat\\\\dis25_abgabe\\\\data\\\\wpdata\\\\data\\\\hundredthousand_wp.pickle'\n",
    "\n",
    "del WpDataSmall[:]\n",
    "WpDataSmall.append(hundredthousandWPpath)\n",
    "WpDataSmall\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08cc81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDfFromSmallFiles(listOfSmallFiles):\n",
    "    \"\"\"\n",
    "    Opens all small-pickle-dataframes and connects them to one big Dataframe.\n",
    "    \n",
    "    args: \n",
    "    listOfSmallFiles = list of the splitted TREC File\n",
    "    \"\"\"\n",
    "    fileending = listOfSmallFiles[0]\n",
    "    fileending = pathlib.Path(fileending).suffix\n",
    "    fileending = fileending[1:]\n",
    "    if fileending == 'pickle':\n",
    "        list_of_dataframes = []\n",
    "        for filename in listOfSmallFiles:\n",
    "            to_merge_df = pd.read_pickle(filename)\n",
    "            list_of_dataframes.append(to_merge_df)\n",
    "\n",
    "        # write all small dataframes in one big dataframe\n",
    "        df = pd.concat(list_of_dataframes)        \n",
    "\n",
    "        # delete the small dataframes for more memory\n",
    "        del to_merge_df\n",
    "        del list_of_dataframes\n",
    "\n",
    "        return df\n",
    "\n",
    "df = createDfFromSmallFiles(WpDataSmall)\n",
    "\n",
    "# Randomize the rows and reset a new id\n",
    "#df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# !!! DATENSATZ VERKLEINERT\n",
    "df = df[:myWishedDatasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58d9ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if myZwischenSpeichern:\n",
    "    zwischenSpeichern(myDataPath, 'v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b2cd73",
   "metadata": {},
   "source": [
    "---\n",
    "# | Cleaning & Transforming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbf3a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df:\n",
    "    df[column] = df[column].replace('', np.nan)\n",
    "    df[column] = df[column].replace('None', np.nan)\n",
    "    df[column] = df[column].replace(r'^\\s*$', np.nan, regex=True)\n",
    "df = df.fillna(value=np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76215233",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dateconvert(x):\n",
    "    try:\n",
    "        import datetime\n",
    "        import numpy as np   \n",
    "        ts = datetime.datetime.fromtimestamp(x.published_date/1000)\n",
    "        # year-month-day with hours-minutes-seconds for future analysis\n",
    "        #date = ts.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        # year-month-day without time\n",
    "        date = ts.strftime('%Y-%m-%d')\n",
    "        return date\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "# converts the epochs datetime to a normale datestring\n",
    "df['published_date'] = df.parallel_apply(dateconvert, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215430a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createText(x):\n",
    "    import numpy as np\n",
    "    try:\n",
    "        import pandas as pd\n",
    "        dfcontent = pd.json_normalize(x.content, max_level=1)\n",
    "        dfcontent = dfcontent[dfcontent.subtype == 'paragraph']\n",
    "        #dfcontent = dfcontent[dfcontent.mime == 'text/html']\n",
    "        contentText = ''\n",
    "        \n",
    "        for index, row in dfcontent.iterrows():\n",
    "            contentText += row[\"content\"]\n",
    "            contentText += ' '\n",
    "            \n",
    "            import re\n",
    "            contentText = contentText.encode('utf-8').decode('ascii','ignore')\n",
    "            contentText = re.sub('<[^<]+?>', '', contentText)\n",
    "            contentText = contentText.replace(u'\\xa0', u' ')\n",
    "            contentText = contentText.replace(f'\\\\n', ' ')\n",
    "            contentText = contentText.replace(f'\\n', f' ')\n",
    "            contentText = contentText.replace('  ',' ')\n",
    "            \n",
    "        return contentText\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "# creates text from the content column by filtering html and json\n",
    "df['text'] = df.parallel_apply(createText, axis=1)\n",
    "\n",
    "# if content json is converted in text delete the column\n",
    "df = df.drop(columns='content')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f1539f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if myZwischenSpeichern:\n",
    "    zwischenSpeichern(myDataPath, 'v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46dbac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combineText(x):\n",
    "    import numpy as np\n",
    "    try:\n",
    "        title = x.title\n",
    "        text = x.text\n",
    "        if title == np.nan:\n",
    "            title = ''\n",
    "        if text == np.nan:\n",
    "            title = ''\n",
    "        mergedText = f'{title} {text}'\n",
    "        return mergedText\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "# write Title and Text in merged_total_text\n",
    "df['merged_total_text'] = df.parallel_apply(combineText, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1ae3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if myZwischenSpeichern:\n",
    "    zwischenSpeichern(myDataPath, 'v3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc06cac",
   "metadata": {},
   "source": [
    "# | Gender Guesser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ff632d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gender_guesser  \n",
    "\n",
    "def authorGender(x):\n",
    "    import numpy as np\n",
    "    try:\n",
    "        import gender_guesser.detector as gender\n",
    "        gd = gender.Detector()\n",
    "        author = x.author\n",
    "        if author == np.nan:\n",
    "            author = ''\n",
    "        author = author.split(' ')[0]\n",
    "        gender = gd.get_gender(author)\n",
    "        return gender\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "df['author_gender'] = df.parallel_apply(authorGender, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7752418",
   "metadata": {},
   "outputs": [],
   "source": [
    "if myZwischenSpeichern:\n",
    "    zwischenSpeichern(myDataPath, 'v4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89db4230",
   "metadata": {},
   "outputs": [],
   "source": [
    "if myZwischenSpeichern:\n",
    "    zwischenSpeichern(myDataPath, 'v5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0c7ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def findCategory(x):\n",
    "    import numpy as np\n",
    "    try:\n",
    "        articleUrl = x.article_url\n",
    "\n",
    "        if articleUrl[:5] == 'https':\n",
    "            category = articleUrl.split('/')[3]\n",
    "        elif articleUrl[:1] == '/':\n",
    "            category = articleUrl.split('/')[1]\n",
    "        else:\n",
    "            category = 'other'\n",
    "        return category\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "# find category from article_url\n",
    "df['category'] = df.parallel_apply(findCategory, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4ecf7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if myZwischenSpeichern:\n",
    "    zwischenSpeichern(myDataPath, 'v6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f30420",
   "metadata": {},
   "outputs": [],
   "source": [
    "def communion_context(x):\n",
    "    import numpy as np\n",
    "    try:\n",
    "        merged_total_text = x.merged_total_text\n",
    "        file = 'a_communion.txt'\n",
    "        with open(file, \"r\") as tf:\n",
    "            lines = tf.read().split('\\n')\n",
    "        import re\n",
    "        counter = 0\n",
    "        for line in lines:\n",
    "            count = sum(1 for _ in re.finditer(r'\\b%s\\b' % re.escape(line), merged_total_text))\n",
    "            counter += count\n",
    "        return counter\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "df['communion_context'] = df.parallel_apply(communion_context, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd1ae55",
   "metadata": {},
   "outputs": [],
   "source": [
    "if myZwischenSpeichern:\n",
    "    zwischenSpeichern(myDataPath, 'v7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00d966f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agency_context(x):\n",
    "    import numpy as np\n",
    "    try:\n",
    "        merged_total_text = x.merged_total_text\n",
    "        file = 'a_agency.txt'\n",
    "        with open(file, \"r\") as tf:\n",
    "            lines = tf.read().split('\\n')\n",
    "        import re\n",
    "        counter = 0\n",
    "        for line in lines:\n",
    "            count = sum(1 for _ in re.finditer(r'\\b%s\\b' % re.escape(line), merged_total_text))\n",
    "            counter += count\n",
    "        return counter\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "df['agency_context'] = df.parallel_apply(agency_context, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14008df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if myZwischenSpeichern:\n",
    "    zwischenSpeichern(myDataPath, 'v8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e2e56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text anstatt Titel\n",
    "def getTextLenght(x):\n",
    "    import numpy as np\n",
    "    try:\n",
    "        return len(x.text)\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "# write length of title\n",
    "df['len_text'] = df.parallel_apply(getTextLenght, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1dc7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if myZwischenSpeichern:\n",
    "    zwischenSpeichern(myDataPath, 'v9')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2161778b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text anstatt Titel\n",
    "def getTextWordLenght(x):\n",
    "    import numpy as np\n",
    "    try:\n",
    "        text = x.text\n",
    "        import spacy\n",
    "        nlp = spacy.load('en_core_web_sm')\n",
    "        text = nlp(text)\n",
    "        tokenizerLength = len(text)\n",
    "        return tokenizerLength\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "# write number of title words\n",
    "df['word_text'] = df.parallel_apply(getTextWordLenght, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa33ecc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if myZwischenSpeichern:\n",
    "    zwischenSpeichern(myDataPath, 'v10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fa04d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text anstatt Titel\n",
    "def TitleVaderScore(x):\n",
    "    import numpy as np\n",
    "    try:\n",
    "        from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "        analyzer = SentimentIntensityAnalyzer()\n",
    "        text = x.text\n",
    "        vader_score = analyzer.polarity_scores(text)['compound']\n",
    "        if vader_score >= 0.05 : \n",
    "            return(\"Positive\") \n",
    "        elif vader_score <= - 0.05 : \n",
    "            return(\"Negative\")  \n",
    "        else : \n",
    "            return(\"Neutral\")\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "# find category from article_url\n",
    "df['compund'] = df.parallel_apply(TitleVaderScore, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fed1f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if myZwischenSpeichern:\n",
    "    zwischenSpeichern(myDataPath, 'v11')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b2f102",
   "metadata": {},
   "source": [
    "# | Named Entity Recognition w/ spaCy NEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a09b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ner_full = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d4ec71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write entity in parsed_text \n",
    "parsed_text = []\n",
    "for text, id in tqdm.tqdm(zip(df_ner_full['merged_total_text'], df_ner_full['id']), total=len(df_ner_full)):\n",
    "    parsed_text.extend([[id, str(entity), entity.label_] for entity in nlp(text).ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ec0a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if myZwischenSpeichern:\n",
    "    zwischenSpeichern(myDataPath, 'v12')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de139667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dfs from entities\n",
    "df_ner = pd.DataFrame(parsed_text, columns=['id', 'merged_total_text', 'entity_type'])\n",
    "df_ner.rename(columns={'merged_total_text': 'entity_per'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1de4b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge back to initial df\n",
    "df_ner_full = (pd.merge(df_ner_full, df_ner, on='id')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993928ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ner_full = (df_ner_full.loc[df_ner_full['entity_type'].isin(['PERSON'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2dd76b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace artist names (Sting, Bono, Cher etc.) with name according to wikipedia\n",
    "# TO do: write into dict?\n",
    "artists = {\n",
    "            'JFK': 'John Fitzgerald Kennedy',\n",
    "            'FDR': 'Franklin Delano Roosevelt',\n",
    "            'Sting': 'Gordon Matthew Sumner',\n",
    "            'Bono': 'Paul David Hewson',\n",
    "            'Cher': 'Cherilyn Sarkisian',\n",
    "            'Madonna': 'Madonna Louise Ciccone',\n",
    "            'Adele': 'Adele Laurie Adkins',\n",
    "            'Eminem': 'Marshall Bruce Mathers',\n",
    "            'Beyonce': 'Beyonc√© Knowles-Carter',\n",
    "            'Blaine Friedlander': 'Blaine P. Friedlander Jr.'\n",
    "        }\n",
    "df_ner_full['entity_per'] = df_ner_full['entity_per'].replace(artists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1517ff95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_name(df, var):\n",
    "    sub_df = df[var].str.split('\\\\s+', expand=True)\n",
    "    result = []\n",
    "\n",
    "    for _, row in sub_df.iterrows():\n",
    "        info = {'first_name': '', 'middle_name': '', 'last_name': ''}\n",
    "        n = row.count()\n",
    "\n",
    "        if n == 0:\n",
    "            pass\n",
    "        elif n == 1:\n",
    "            info['first_name'] = row.iloc[0]\n",
    "        elif n == 2:\n",
    "            info['first_name'], info['last_name'] = row.iloc[:2]\n",
    "        else:\n",
    "            info['first_name'] = row.iloc[0]\n",
    "            info['last_name'] = row.iloc[-1]\n",
    "            info['middle_name'] = ' '.join([(string or '') for string in row.iloc[1:-1]])\n",
    "        result.append(info)\n",
    "    return pd.DataFrame(result, index=df.index)\n",
    "\n",
    "df_names = split_name(df_ner_full, 'entity_per')\n",
    "df_ner_full = df_ner_full.join(df_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3a5ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gender_guesser    \n",
    "import gender_guesser.detector as gender\n",
    "gd = gender.Detector()\n",
    "df_ner_full['gender_guesser'] = df_ner_full['first_name'].apply(str.capitalize).map(lambda x: gd.get_gender(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e70c536",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.drop(['agency_context', 'len_title', 'word_title', 'compund', 'content', 'communion_context', 'category' , 'entitys'], axis=1, inplace=True)#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043fb9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ner_full.gender_guesser.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a90562",
   "metadata": {},
   "outputs": [],
   "source": [
    "if myZwischenSpeichern:\n",
    "    zwischenSpeichern(myDataPath, 'v13')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28c6752",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ner_full"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26280c41",
   "metadata": {},
   "source": [
    "# | Auswertung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19a1201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laden des aktuellen Stands\n",
    "#aktuellPickle = 'C:\\\\Users\\\\Felix\\\\github\\\\privat\\\\dis25_abgabe\\\\data\\\\wpdata\\\\data\\\\aktuell.pickle'\n",
    "#df = pd.read_pickle(aktuellPickle)\n",
    "#df = df_ner_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35865ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"author_gender\")[\"id\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9408d93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Es wurden nur 280 AutorInnen ein Gender zugewiesen\n",
    "df[\"author_gender\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6de217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insgesamt besteht der Datensatz aktuell aus 300 Zeilen\n",
    "df[\"id\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54820a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "gender = [\"andy\", \"female\", \"male\", \"mostly_female\", \"mostly_male\", \"unknown\"]\n",
    "other = [\"andy\", \"mostly_female\", \"mostly_male\", \"unknown\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbee0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Artikel denen Gender \"None\" zugeordnet wurde\n",
    "df[~df.author_gender.isin(gender)][\"id\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5e6b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# None Werte aus Spalte 'author_gender' durch 'unknown' ersetzen\n",
    "df['author_gender'] = df['author_gender'].replace({None: 'unknown'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d614d441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anzahl Artikel gesamt\n",
    "'Im Datensatz sind ingesamt ' + str(df[\"id\"].count()) + ' Artikel vorhanden'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e80bdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anzahl AutorenInnen\n",
    "'Die ' + str(df[\"id\"].count()) + ' Artikel wurden von insgesamt ' + str(df[\"author\"].nunique()) + ' AutorInnen verfasst'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b215c83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anzahl AutorenInnen nach Gender\n",
    "df.groupby('author_gender')[\"id\"].count().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bf34de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('author_gender')[\"id\"].count().sort_values(ascending=False).plot(kind='bar')\n",
    "plt.title('Anzahl AutorInnen nach Geschlecht')\n",
    "plt.xlabel('Geschlecht')\n",
    "plt.ylabel('Anzahl Artikel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48b3f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "'Die Grafik zeigt, dass ' \\\n",
    "+ str(df[df[\"author_gender\"] == 'male'][\"id\"].count()) + ' Artikel von M√§nnern, ' \\\n",
    "+ str(df[df[\"author_gender\"] == 'female'][\"id\"].count()) + ' von Frauen und ' \\\n",
    "+ str(df[df[\"author_gender\"].isin(other)][\"id\"].count()) + ' vom Gender Guesser nicht klar zuordenbaren Geschlechtern verfasst worden sind.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d855d237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anzahl Artikel pro AutorIn nach Gender\n",
    "'M√§nnliche Autoren verfassen bei der Washington Post im Durchschnitt ' + str(round(df[df[\"author_gender\"] == 'male'].groupby(\"author\")[\"id\"].count().mean(), 2)) + ' Artikel, ' \\\n",
    "+ 'Frauen hingegen im Durschnitt ' + str(round(df[df[\"author_gender\"] == 'female'].groupby(\"author\")[\"id\"].count().mean(), 2)) + ' Artikel'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e44a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vader Stimmungsanalyse Auswertung (absolut) - M√§nnliche Autoren\n",
    "df[df[\"author_gender\"] == 'male'].groupby(['compund'])[\"compund\"].count().sort_values(ascending=False).plot(kind='bar')\n",
    "plt.title('Auswertung Stimmungsanalyse bei Artikeln von m√§nnlichen Autoren (absolut)')\n",
    "plt.xlabel('Stimmung')\n",
    "plt.ylabel('Anzahl Artikel')\n",
    "plt.ylim([10, 80])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbc718b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vader Stimmungsanalyse Auswertung (relativ) - M√§nnliche Autoren\n",
    "vader_male = pd.DataFrame(df[df[\"author_gender\"] == 'male'].groupby(['compund'])[\"id\"].count().sort_values(ascending=False))\n",
    "vader_male[\"total\"] = df[df[\"author_gender\"] == 'male'][\"id\"].count()\n",
    "vader_male[\"relative\"] = vader_male[\"id\"] / vader_male[\"total\"] * 100\n",
    "vader_male[\"relative\"].sort_values(ascending=False).plot(kind='pie', autopct='%1.1f%%')\n",
    "plt.title('Auswertung Stimmungsanalyse bei Artikeln von m√§nnlichen Autoren (relativ)')\n",
    "plt.xlabel(\"\")\n",
    "plt.ylabel(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5ed631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vader Stimmungsanalyse Auswertung (absolut) - Weibliche Autorinnen\n",
    "df[df[\"author_gender\"] == 'female'].groupby(['compund'])[\"compund\"].count().sort_values(ascending=False).plot(kind='bar')\n",
    "plt.title('Auswertung Stimmungsanalyse bei Artikeln von weiblichen Autorinnen (absolut)')\n",
    "plt.xlabel('Stimmung')\n",
    "plt.ylabel('Anzahl Artikel')\n",
    "plt.ylim([10, 80])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9e96aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vader Stimmungsanalyse Auswertung (relativ) - Weibliche Autorinnen\n",
    "vader_female = pd.DataFrame(df[df[\"author_gender\"] == 'female'].groupby(['compund'])[\"id\"].count().sort_values(ascending=False))\n",
    "vader_female[\"total\"] = df[df[\"author_gender\"] == 'female'][\"id\"].count()\n",
    "vader_female[\"relative\"] = vader_female[\"id\"] / vader_female[\"total\"] * 100\n",
    "vader_female[\"relative\"].sort_values(ascending=False).plot(kind='pie', autopct='%1.1f%%')\n",
    "plt.title('Auswertung Stimmungsanalyse bei Artikeln von weibliche Autorinnen (relativ)')\n",
    "plt.xlabel(\"\")\n",
    "plt.ylabel(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98e4af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vader Stimmungsanalyse Auswertung (relativ) - AutorInnen im Vergleich (Datentabelle)\n",
    "vader_male = vader_male.rename(columns={\"id\": \"vader_male_count\", \"total\": \"vader_male_total\", \"relative\": \"vader_male_relative\"})\n",
    "vader_female = vader_female.rename(columns={\"id\": \"vader_female_count\", \"total\": \"vader_female_total\", \"relative\": \"vader_female_relative\"})\n",
    "vader_total = pd.concat([vader_male,vader_female], axis = 1, sort = False)\n",
    "vader_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff01def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vader Stimmungsanalyse Auswertung (relativ) - AutorInnen im Vergleich (Plot)\n",
    "vader_total[[\"vader_male_relative\", \"vader_female_relative\"]].plot(kind='bar')\n",
    "plt.title('Auswertung Stimmungsanalyse bei Artikeln von AutorInnen (relativ)')\n",
    "plt.xlabel('Stimmung')\n",
    "plt.ylabel('Anteil Artikel (in%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76ef9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 5 Kateogorien (absolut) - M√§nnliche Autoren\n",
    "df[df.author_gender == 'male'].groupby(['category'])[\"id\"].count().sort_values(ascending=False).head(5).plot(kind='bar')\n",
    "plt.title('Top 5 Kategorien von m√§nnlichen Autoren (absolut)')\n",
    "plt.xlabel('Kategorie')\n",
    "plt.ylabel('Anzahl Artikel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07fff73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 5 Kateogorien (relativ) - M√§nnliche Autoren\n",
    "top_male = pd.DataFrame(df[df.author_gender == 'male'].groupby(['category'])[\"id\"].count().sort_values(ascending=False).head(5))\n",
    "top_male[\"total\"] = df[df[\"author_gender\"] == 'male'][\"id\"].count()\n",
    "top_male[\"relative\"] = top_male[\"id\"] / top_male[\"total\"] * 100\n",
    "top_male[\"relative\"].sort_values(ascending=False).plot(kind='pie', autopct='%1.1f%%')\n",
    "plt.title('Top 5 Kategorien von m√§nnlichen Autoren (relativ)')\n",
    "plt.xlabel(\"\")\n",
    "plt.ylabel(\"\")\n",
    "\n",
    "# Info: Prozentualer Anteil auf Basis der Top 5 Kategorien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8385c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 5 Kateogorien - Weibliche Autorinnen\n",
    "df[df.author_gender == 'female'].groupby(['category'])[\"id\"].count().sort_values(ascending=False).head(5).plot(kind='bar')\n",
    "plt.title('Top 5 Kategorien von weiblichen Autorinnen (absolut)')\n",
    "plt.xlabel('Kategorie')\n",
    "plt.ylabel('Anzahl Artikel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d150348b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 5 Kateogorien (relativ) - Weibliche Autoreninnen\n",
    "top_female = pd.DataFrame(df[df.author_gender == 'female'].groupby(['category'])[\"id\"].count().sort_values(ascending=False).head(5))\n",
    "top_female[\"total\"] = df[df[\"author_gender\"] == 'female'][\"id\"].count()\n",
    "top_female[\"relative\"] = top_female[\"id\"] / top_female[\"total\"] * 100\n",
    "top_female[\"relative\"].sort_values(ascending=False).plot(kind='pie', autopct='%1.1f%%')\n",
    "plt.title('Top 5 Kategorien von weiblichen Autoreninnen (relativ)')\n",
    "plt.xlabel(\"\")\n",
    "plt.ylabel(\"\")\n",
    "\n",
    "# Info: Prozentualer Anteil auf Basis der Top 5 Kategorien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee94f83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auswertung L√§nge der Artikel/Texte und Speicherung unter neuer Spalte 'len_text'\n",
    "df[\"len_text\"] = df.apply(lambda row: len(str(row[\"text\"])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcaa1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Durchschnittliche W√∂rter nach Geschlecht\n",
    "df[~df.author_gender.isin(other)].groupby(['author_gender'])[\"len_text\"].mean().plot(kind='barh')\n",
    "plt.title('Durschnittliche Anzahl von W√∂rtern von Washington Post Artikeln')\n",
    "plt.xlabel('Geschlecht')\n",
    "plt.ylabel('Durschnittliche Anzahl W√∂rter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1268752d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'Die Grafik zeigt, dass' \\\n",
    "+ ' m√§nnliche Autoren im Durchschnitt ' +  str(round(df[df[\"author_gender\"] == 'male'][\"len_text\"].mean(),2)) + ' W√∂rter verfassen' \\\n",
    "+ ' und weibliche Autorinnen im Durchschnitt ' +  str(round(df[df[\"author_gender\"] == 'female'][\"len_text\"].mean(),2)) + ' W√∂rter verfassen '"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba1d23b",
   "metadata": {},
   "source": [
    "### Gender Guesser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11eb86b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anzahl von Gender-Erw√§hnungen in Artikeln pro Author Gender\n",
    "df_ner_full[df_ner_full.author_gender == 'male'].groupby(['gender_guesser'])[\"gender_guesser\"].count().sort_values(ascending=False).plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31000c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anzahl von Gender-Erw√§hnungen in Artikeln pro Author Gender\n",
    "df_ner_full[df_ner_full.author_gender == 'female'].groupby(['gender_guesser'])[\"gender_guesser\"].count().sort_values(ascending=False).plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18aedd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "male_guesser = pd.DataFrame(df_ner_full[df_ner_full.author_gender == 'male'].groupby(['gender_guesser'])[\"id\"].count())\n",
    "male_guesser = male_guesser.rename(columns={\"id\": \"count_male\"})\n",
    "female_guesser = pd.DataFrame(df_ner_full[df_ner_full.author_gender == 'female'].groupby(['gender_guesser'])[\"id\"].count())\n",
    "female_guesser = female_guesser.rename(columns={\"id\": \"count_female\"})\n",
    "total_guesser = pd.concat([male_guesser,female_guesser], axis = 1, sort = False)\n",
    "total_guesser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e51fa77",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_guesser = total_guesser.drop(other)\n",
    "total_guesser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62130ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_guesser[\"total_male\"] = df_ner_full[df_ner_full.author_gender == 'male'][\"id\"].count()\n",
    "total_guesser[\"relative_male\"] = total_guesser[\"count_male\"] / total_guesser[\"total_male\"] * 100\n",
    "total_guesser[\"total_female\"] = df_ner_full[df_ner_full.author_gender == 'female'][\"id\"].count()\n",
    "total_guesser[\"relative_female\"] = total_guesser[\"count_female\"] / total_guesser[\"total_female\"] * 100\n",
    "total_guesser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9ceb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_guesser[[\"count_male\", \"count_female\"]].sort_values(by='count_male', ascending=False).plot(kind=\"bar\")\n",
    "plt.title('Identifizierung von erw√§hnten Personen in Artikeln (absolut)')\n",
    "plt.xlabel('Geschlecht')\n",
    "plt.ylabel('Anzahl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fceb104d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "total_guesser[[\"relative_male\", \"relative_female\"]].sort_values(by='relative_male', ascending=False).plot(kind=\"pie\", subplots=True, autopct='%1.1f%%')\n",
    "plt.title('Identifizierung von erw√§hnten Personen in Artikeln (relative)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23052de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sum Communion Male:\" + str(df[df[\"author_gender\"] == 'male'][\"communion_context\"].sum()))\n",
    "print(\"Sum Agency Male:\" + str(df[df[\"author_gender\"] == 'male'][\"agency_context\"].sum()))\n",
    "\n",
    "print(\"Sum Communion Female:\" + str(df[df[\"author_gender\"] == 'female'][\"communion_context\"].count()))\n",
    "print(\"Sum Agency Female:\" + str(df[df[\"author_gender\"] == 'female'][\"agency_context\"].count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15852c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Relativer Anteiler Communion Male:\" + str(df[df[\"author_gender\"] == 'male'][\"communion_context\"].sum() / (df[df[\"author_gender\"] == 'male'][\"communion_context\"].sum() + df[df[\"author_gender\"] == 'male'][\"agency_context\"].sum()) * 100))\n",
    "print(\"Relativer Anteiler Agency Male:\" + str(df[df[\"author_gender\"] == 'male'][\"agency_context\"].sum() / (df[df[\"author_gender\"] == 'male'][\"communion_context\"].sum() + df[df[\"author_gender\"] == 'male'][\"agency_context\"].sum()) * 100))\n",
    "\n",
    "print(\"Relativer Anteiler Communion Female:\" + str(df[df[\"author_gender\"] == 'female'][\"communion_context\"].sum() / (df[df[\"author_gender\"] == 'female'][\"communion_context\"].sum() + df[df[\"author_gender\"] == 'female'][\"agency_context\"].sum()) * 100))\n",
    "print(\"Relativer Anteiler Agency Female:\" + str(df[df[\"author_gender\"] == 'female'][\"agency_context\"].sum() / (df[df[\"author_gender\"] == 'female'][\"communion_context\"].sum() + df[df[\"author_gender\"] == 'female'][\"agency_context\"].sum()) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996a935c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gr√ºnde f√ºr Unkown aufzeigen\n",
    "# Was ist wenn mehrere Persons in Artikel? Frage an Julia\n",
    "# Gender None , warum?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633ea219",
   "metadata": {},
   "outputs": [],
   "source": [
    "if myZwischenSpeichern:\n",
    "    zwischenSpeichern(myDataPath, 'v14')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdb9e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "zeitende = time.time()\n",
    "zeitdauer = zeitende-zeitanfang\n",
    "zeitdauer = time.strftime('%H:%M:%S', time.gmtime(zeitdauer))\n",
    "\n",
    "print(f'Dauer Programmausf√ºhrung:\\n{zeitdauer}\\nbei {myWishedDatasets} Datens√§tzen ----------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cf2aed",
   "metadata": {},
   "source": [
    "## Snorkel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7879956e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install snorkel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743c90a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from snorkel.labeling import labeling_function\n",
    "from snorkel.labeling import PandasLFApplier\n",
    "from snorkel.labeling.model import LabelModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97158f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled = df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615c6d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train,df_test = train_test_split(df, train_size=0.7)\n",
    "print(df_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49295df1",
   "metadata": {},
   "source": [
    "#### Regeln f√ºr Unknown in Autor:\n",
    "\n",
    "M√§nner schreiben mehr √ºber Sports und Politics\n",
    "\n",
    "Frauen schreiben mehr √ºber Lifestyle und Blogs\n",
    "\n",
    "Texte von M√§nnern sind l√§nger als die von Frauen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712e6ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "MALE_AUTHOR = 1\n",
    "FEMALE_AUTHOR = -1\n",
    "UNIDENTIFIED_AUTHOR = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d354355",
   "metadata": {},
   "outputs": [],
   "source": [
    "@labeling_function()\n",
    "def lf_category_lookup(x):\n",
    "    keywords_male = [\"sports\", \"politics\"]\n",
    "    keywords_female = [\"blogs\", \"lifestyle\"]\n",
    "    if any(word in x.category.lower() for word in keywords_male):\n",
    "        return MALE_AUTHOR\n",
    "    elif any(word in x.category.lower() for word in keywords_female):\n",
    "        return FEMALE_AUTHOR\n",
    "    else:\n",
    "        return UNIDENTIFIED_AUTHOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c582fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@labeling_function()\n",
    "def lf_word_lookup(x):\n",
    "    import numpy as np\n",
    "    try:\n",
    "        text = x.text\n",
    "        import spacy\n",
    "        nlp = spacy.load('en_core_web_sm')\n",
    "        text = nlp(text)\n",
    "        tokenizerLength = len(text)\n",
    "        if tokenizerLength >= 3900:\n",
    "            return MALE_AUTHOR\n",
    "        else:\n",
    "            return FEMALE_AUTHOR\n",
    "    except:\n",
    "        return np.nan   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400da74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lfs = [lf_category_lookup,lf_word_lookup]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7c2956",
   "metadata": {},
   "outputs": [],
   "source": [
    "applier = PandasLFApplier(lfs=lfs)\n",
    "L_train = applier.apply(df=df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb9aa6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Erst aufrufen, wenn wir eine dritte Funktion zur Unterscheidung haben\n",
    "label_model = LabelModel(cardinality=2,verbose=True)\n",
    "#label_model.fit(L_train=L_train,n_epochs=500,log_freq=100,seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1de6448",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Labels_Author'] = label_model.predict(L=L_train,tie_break_policy=\"unindetified\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f12d7a",
   "metadata": {},
   "source": [
    "#### Regeln f√ºr Gender innerhalb der Texte:\n",
    "\n",
    "M√§nner werden mehr mit Agency W√∂rtern beschrieben\n",
    "\n",
    "Frauen werden mit mehr Community W√∂rtern beschrieben\n",
    "\n",
    "\"she\" ist weiblich, \"he\" ist m√§nnlich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4add340d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MALE_TEXT = 1\n",
    "FEMALE_TEXT = -1\n",
    "UNIDENTIFIED_TEXT = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3582c924",
   "metadata": {},
   "outputs": [],
   "source": [
    "@labeling_function()\n",
    "def lf_prenoms_lookup(x):\n",
    "    keywords_male = [\"he\", \"his\", \"him\"]\n",
    "    keywords_female = [\"he\", \"his\", \"him\"]\n",
    "    if any(word in x.text.lower() for word in keywords_male):\n",
    "        return MALE_TEXT\n",
    "    elif any(word in x.text.lower() for word in keywords_female):\n",
    "        return FEMALE_TEXT\n",
    "    else:\n",
    "        return UNIDENTIFIED_TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c47472",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "lfs_text = [lf_prenoms_lookup]\n",
    "applier_text = PandasLFApplier(lfs=lfs_text)\n",
    "L_train = applier.apply(df=df_train)\n",
    "label_model = LabelModel(cardinality=2,verbose=True)\n",
    "#label_model.fit(L_train=L_train,n_epochs=500,log_freq=100,seed=123)\n",
    "df_train['Labels_Persons'] = label_model.predict(L=L_train,tie_break_policy=\"unindetified\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3974fae",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "---\n",
    "\n",
    "# | UNFERTIGE FUNKTIONEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0391df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# id\n",
    "# article_url\n",
    "# title\n",
    "# author\n",
    "# published_date\n",
    "# type\n",
    "# source\n",
    "# content\n",
    "# publish_date\n",
    "# orig-id\n",
    "# text\n",
    "# merged_total_text\n",
    "# entitys\n",
    "\n",
    "# df[df['publish_date'].isnull()]\n",
    "\n",
    "def createEntitys(x):\n",
    "    import spacy\n",
    "    import en_core_web_trf\n",
    "\n",
    "    # TRF Orininal / Md schneller\n",
    "    nlp = spacy.load(\"en_core_web_md\")\n",
    "    # JULIA nlp = spacy.load(\"en_core_web_trf\")\n",
    "    \n",
    "    totalText = x['merged_total_text']\n",
    "    try:\n",
    "        entitys = {}\n",
    "        for word in nlp(totalText).ents:\n",
    "        # WENN WORD.LABEL_ IS PERSON DANN NUR HINZUF√úGEN?!\n",
    "            entitys[str(word)] = word.label_\n",
    "        return entitys\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "#x = df.entitys[0]\n",
    "#for word, entity in x.items():\n",
    "#    if entity == 'PERSON':\n",
    "#        print(f'{word} is {entity}')\n",
    "\n",
    "#df_ner_full.head(100)\n",
    "\n",
    "\n",
    "# WIRD IN DER ANALYSE NICHT BEN√ñTIGT\n",
    "# df['entitys'] = df.parallel_apply(createEntitys, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7e3728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NER 'PERSON' STARTS HERE\n",
    "#df = (df.loc[df['entity_type'].isin(['PERSON'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f4aefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[\"entity\"] = df['entity'].str.replace('[\\[\\]\\\"\\'\\d\\,\\<\\/]','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ca2615",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createEntitys(x):\n",
    "    import spacy\n",
    "    import en_core_web_trf\n",
    "\n",
    "    # TRF Orininal / Md schneller\n",
    "    nlp = spacy.load(\"en_core_web_md\")\n",
    "    # JULIA nlp = spacy.load(\"en_core_web_trf\")\n",
    "    \n",
    "    totalText = x['merged_total_text']\n",
    "    try:\n",
    "        entitys = {}\n",
    "        for word in nlp(totalText).ents:\n",
    "        # WENN WORD.LABEL_ IS PERSON DANN NUR HINZUF√úGEN?!\n",
    "            entitys[str(word)] = word.label_\n",
    "        return entitys\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "#x = df.entitys[0]\n",
    "#for word, entity in x.items():\n",
    "#    if entity == 'PERSON':\n",
    "#        print(f'{word} is {entity}')\n",
    "\n",
    "#df_ner_full.head(100)\n",
    "\n",
    "\n",
    "# WIRD IN DER ANALYSE NICHT BEN√ñTIGT\n",
    "# df['entitys'] = df.parallel_apply(createEntitys, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506bc2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace artist names (Sting, Bono, Cher etc.) with name according to wikipedia\n",
    "def replaceEntityNames(x):\n",
    "    try:\n",
    "        entitys = x.entitys\n",
    "        artists = {\n",
    "            'JFK': 'John Fitzgerald Kennedy',\n",
    "            'FDR': 'Franklin Delano Roosevelt',\n",
    "            'Sting': 'Gordon Matthew Sumner',\n",
    "            'Bono': 'Paul David Hewson',\n",
    "            'Cher': 'Cherilyn Sarkisian',\n",
    "            'Madonna': 'Madonna Louise Ciccone',\n",
    "            'Adele': 'Adele Laurie Adkins',\n",
    "            'Eminem': 'Marshall Bruce Mathers',\n",
    "            'Beyonce': 'Beyonc√© Knowles-Carter',\n",
    "            'Blaine Friedlander': 'Blaine P. Friedlander Jr.'\n",
    "        }\n",
    "\n",
    "        for entityKey, entityValue in entitys.items():\n",
    "            if entityValue == 'PERSON':\n",
    "                for artAbkzg, artName in artists.items():\n",
    "                    if entityKey == artAbkzg:\n",
    "                        entitys[artName]=entityValue\n",
    "                        del entitys[entityKey]  \n",
    "    \n",
    "        return entitys\n",
    "    except:\n",
    "        return x.entitys\n",
    "# !!! NOCH FEHLERHAFT\n",
    "#df['entitys'] = df.parallel_apply(replaceEntityNames, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccc2347",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df[df['author'] != df['entity']] # if author is in col entity -> remove, we only want \"Person\" from within the article\n",
    "# To do: Blaine P. Friedlander Jr. vs. Blaine Friedlander "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadf52c8",
   "metadata": {},
   "source": [
    "## Split first, middle and last name in different cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53e317d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write first, middle and last name in different cols\n",
    "# to do: split middle and last name does not work\n",
    "def split_name(df, var):\n",
    "    sub_df = df[var].str.split('\\\\s+', expand=True)\n",
    "    result = []\n",
    "\n",
    "    for _, row in sub_df.iterrows():\n",
    "        info = {'first_name': '', 'middle_name': '', 'last_name': ''}\n",
    "        n = row.count()\n",
    "\n",
    "        if n == 0:\n",
    "            pass\n",
    "        elif n == 1:\n",
    "            info['last_name'] = row.iloc[0]\n",
    "        elif n == 2:\n",
    "            info['first_name'], info['last_name'] = row.iloc[:2]\n",
    "        else:\n",
    "            info['first_name'] = row.iloc[0]\n",
    "            info['last_name'] = row.iloc[-1]\n",
    "            info['middle_name'] = ' '.join([(string or '') for string in row.iloc[1:-1]])\n",
    "        result.append(info)\n",
    "    return pd.DataFrame(result, index=df.index)\n",
    "\n",
    "df_names = split_name(df, 'entitys')\n",
    "df = df.join(df_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb19388",
   "metadata": {},
   "source": [
    "## | Enrich first name if only last name is mentioned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66dcbc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = df.groupby('id')['entity'].apply(lambda x: list(np.unique(x)))\n",
    "# To do: if only one token (\"Obama\") drop or don't write to list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb71d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for enriching first name if only last name is mentioned\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "def enrich_firstname(row):\n",
    "    \n",
    "    entity = row['entity']#.copy()\n",
    "    firstname = row['first_name']#.copy()\n",
    "    lastname = row['last_name']#.copy()\n",
    "    \n",
    "    #if len(firstname) == 0:\n",
    "        # look in list per id for key and write value?\n",
    "    # elif \n",
    "    # bestehende Vornamen nicht √ºberschreiben\n",
    "    #else:\n",
    "    #    row['first_name'] = 'unknown'\n",
    "    return row\n",
    "\n",
    "df_test = df_test.progress_apply(enrich_firstname, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6ede2e",
   "metadata": {},
   "source": [
    "# | Gender Guesser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d59f6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gender_guesser    \n",
    "import gender_guesser.detector as gender\n",
    "gd = gender.Detector()\n",
    "df['gender_guesser'] = df['first_name'].apply(str.capitalize).map(lambda x: gd.get_gender(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580d078e",
   "metadata": {},
   "source": [
    "> __unknown__ (name not found), __andy__ (androgynous), __male__, __female__, __mostly_male__, or __mostly_female__. The difference between andy and unknown is that the former is found to have the same probability to be male than to be female, while the later means that the name wasn‚Äôt found in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24be056b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.gender_guesser.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23404e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['gender_guesser'] == 'unknown'][:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f5d075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.first_name.value_counts()[:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b1516c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to do:\n",
    "# Namen cleanen\n",
    "# weitere lib f√ºr gender identification einbauen\n",
    "# function schreiben: wenn nur Nachname genannt, schaue in Liste/anderer Zeile pro Artikel, ob Name schon genannt, dann Vorname auff√ºllen\n",
    "# oder\n",
    "# wenn Obama, Santorum, Gingrich, etc. dann aus erstellter Liste oder via Wikidata mit Vornamen auff√ºllen\n",
    "# # dominique rodgers cromartie etc. (famous people)  -> Wikidata? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76df8496",
   "metadata": {},
   "source": [
    "---\n",
    "# | Group per article and get share of gender per article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9598d011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c40063",
   "metadata": {},
   "source": [
    "# | Wikidata Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625c1708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# oder das https://stackoverflow.com/questions/51419785/extract-data-from-wikidata-in-python\n",
    "\n",
    "import requests\n",
    "\n",
    "sparql_query = \"\"\"\n",
    "        prefix schema: <http://schema.org/>\n",
    "        SELECT ?item ?occupation ?genderLabel ?bdayLabel\n",
    "        WHERE {\n",
    "            <https://en.wikipedia.org/wiki/Angela_Merkel> schema:about ?item .\n",
    "            ?item wdt:P21 ?gender .\n",
    "            SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\" }\n",
    "        }\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a50b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://query.wikidata.org/sparql'\n",
    "\n",
    "# sleep(2)\n",
    "r = requests.get(url, params={'format': 'json', 'query': sparql_query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec87c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://query.wikidata.org/sparql'\n",
    "\n",
    "r = requests.get(url, params={'format': 'json', 'query': sparql_query})\n",
    "data = r.json()\n",
    "\n",
    "print(data['results']['bindings'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
